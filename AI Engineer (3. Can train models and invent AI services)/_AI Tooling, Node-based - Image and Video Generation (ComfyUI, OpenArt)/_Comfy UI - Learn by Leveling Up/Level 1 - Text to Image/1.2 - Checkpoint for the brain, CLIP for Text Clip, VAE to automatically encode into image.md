Continued explaining the other nodes:
![[Pasted image 20250615072843.png]]


## âœ… **1. `Load Checkpoint`** â€“ Load a Model

This loads a **Stable Diffusion model checkpoint** (`.ckpt` or `.safetensors`) â€” itâ€™s like choosing which â€œbrainâ€ the AI uses to generate.

### Options:

|Option|Explanation|
|---|---|
|**ckpt_name**|Choose the model file (e.g., `v1-5-pruned.safetensors`, `sd_xl_base_1.0.safetensors`)|
|**vae_name**|Optional: overrides VAE (color and contrast post-processing). If empty, uses the modelâ€™s default VAE.|

> [!note] ğŸ” What's a Checkpoint?  
> A checkpoint is a trained model. It determines the art style, quality, and what kinds of images it can generate. Think of it as the "artist's personality."

> [!note] Filenaming conventions
> The checkpoints and models don't follow a strict naming convention because no one's enforcing them, however the community still attempts to convey basic stats in the filename. In this format `v-1-5-pruned-emaonly-fp16.safetensors`
> - EMA = exponential moving average. Instead of containing raw weights that are updated on every training step, the model contains EMA weights that are smoothed average of the past weights. This reduces file size and also reduces artifacts.
> - FP16 means 16-bit floating point precision. This is a way numbers are stored and calculated inside AI models that uses half the memory of standard 32-floats (FP32). This makes the model more accessible to older CPUs.
>


---

## ğŸ§  **2. `CLIP Text Encode`** â€“ Encode the Prompt

This converts your **text prompt into a vector** that the model understands.

### Options:

|Option|Explanation|
|---|---|
|**Text**|Your prompt (e.g., `"a cute fox in a forest"`)|
|**CLIP**|Connect this from the `Load Checkpoint` node (donâ€™t touch unless using multiple encoders)|

Youâ€™ll use **two `CLIP Text Encode` nodes** â€” one for the prompt, one for the **negative prompt** (what to avoid).

More technically nuanced explanation:
CLIP standas for ==**[Contrastive Language-Image Pre-training](https://www.google.com/search?q=Contrastive+Language-Image+Pre-training&rlz=1C5CHFA_enUS1017US1017&oq=what+clip+stands+for+in+comfyui&gs_lcrp=EgZjaHJvbWUyBggAEEUYOdIBCDMyMjNqMGo0qAIAsAIA&sourceid=chrome&ie=UTF-8&ved=2ahUKEwiLy-ev0OySAxUflmoFHaldCBYQgK4QegQIARAE)**==. It is an OpenAI-developed model that acts as a translator, converting text prompts into numerical representations (embeddings) that the Stable Diffusion model understands to generate images.

---

## ğŸŒˆ **3. `KSampler`** â€“ Generates the Image

(Already explained in [[1.1 - KSampler for diffusion, Latent Image the canvas]].)

> It combines model + prompt + image size + noise seed to generate your image over time.

---

## ğŸ“„ **4. `Empty Latent Image`** â€“ Blank Image Space

(Already explained in [[1.1 - KSampler for diffusion, Latent Image the canvas]].)

> This sets the width and height of your image before itâ€™s turned into pixels.

---

## ğŸ–¼ï¸ **5. `VAE Decode`** â€“ Turns Latent into Visible Image


Stands for Variation AutoEncoder

After the image is generated in a **latent space**, this node converts it into an **actual image you can see and save.**

In other words: The Load VAE node can be used to load a specific VAE model. VAE models are used to encode and decode images to and from latent space.

> [!note] ğŸ¨ What's a VAE and latent space?  
> A VAE (Variational AutoEncoder) is a part of Stable Diffusion that translates the â€œinvisible math imageâ€ into something visual. Different VAEs can slightly affect sharpness, color, and contrast.

### Options:

|Option|Explanation|
|---|---|
|**Samples**|Connect from `KSampler`|
|**VAE**|Connect from `Load Checkpoint` or your own VAE file|

---

## ğŸ’¾ **6. `Save Image`** â€“ Save to Disk

This saves the final output to a folder on your system.

### Options:

|Option|Explanation|
|---|---|
|**filename_prefix**|Optional: lets you label the output images (e.g., `fox_`)|
|**images**|Connect this from the `VAE Decode` node|

---

## ğŸ§ª Simple Example Workflow

```
Load Checkpoint
   â†“
CLIP Text Encode (Prompt)
CLIP Text Encode (Negative Prompt)
   â†“
Empty Latent Image â†’ KSampler
   â†“
VAE Decode
   â†“
Save Image
```

This chain creates a fully working text-to-image pipeline.

---

Weng's personal notes:
https://chatgpt.com/c/684ed776-1914-800f-bf1c-7d82238ea592