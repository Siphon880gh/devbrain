## ğŸ”¢ Model Checkpoint Ranking (Light â†’ Heavy)

| Rank            | Model Type                            | Example Checkpoints                    | VRAM Needed (GPU) | CPU-Only Feasible?        | Who Itâ€™s For              |
| --------------- | ------------------------------------- | -------------------------------------- | ----------------- | ------------------------- | ------------------------- |
| ğŸŸ¢ 1 (Lightest) | **SD 1.5 (fp16)**                     | `v1-5-pruned-emaonly-fp16.safetensors` | 4â€“6 GB            | âš ï¸ Very slow but possible | Most laptops w/ basic GPU |
| ğŸŸ¢ 2            | **SD 1.5 (fp32)**                     | `v1-5-pruned-emaonly.safetensors`      | 6â€“8 GB            | âŒ Not practical           | Older GPUs                |
| ğŸŸ¡ 3            | **SD 2.1**                            | `v2-1_768-ema-pruned.safetensors`      | 6â€“8 GB            | âŒ Not practical           | Mid-range GPUs            |
| ğŸŸ¡ 4            | **SDXL Base**                         | `sd_xl_base_1.0.safetensors`           | 8â€“12 GB           | âŒ No                      | Modern GPUs (3060+)       |
| ğŸŸ¡ 5            | **SDXL + Refiner**                    | `sd_xl_refiner_1.0.safetensors`        | 12â€“16 GB          | âŒ No                      | 12GB+ cards               |
| ğŸ”´ 6            | **Flux Dev**                          | `flux-dev.safetensors`                 | 16â€“24 GB          | âŒ No                      | 3090 / 4090               |
| ğŸ”´ 7 (Heaviest) | **Flux Pro / Large Diffusion Models** | `flux-pro`, 20B+ models                | 24GB+             | âŒ No                      | Enterprise GPUs           |

**TLDR:**
SD 1.5 is lighter than SDXL/Flux-style workflows, so itâ€™s more realistic for average local machines (especially older GPUs / lower VRAM / Macs).

---

## ğŸ’» What This Means Practically

### ğŸŸ¢ SD 1.5 (Best for Most Computers)

- Fastest startup
    
- Lowest VRAM use
    
- Works on:
    
    - 6GB GPUs (GTX 1660, 2060)
        
    - Many 8GB GPUs
        
    - M1/M2 Macs (with patience)
        
- Best for stable local ComfyUI learning
    

---

### ğŸŸ¡ SDXL

- Much heavier
    
- Needs:
    
    - 8GB minimum (tight)
        
    - 12GB recommended
        
- Slower on Macs
    
- Better realism but more crashes on low VRAM
    

---

### ğŸ”´ Flux Models

- Extremely heavy transformer architecture
    
- Designed for:
    
    - 16GB+ GPUs minimum
        
    - 24GB ideal
        
- Not beginner-friendly locally
    
- Often better suited for cloud GPUs
    

---

## ğŸ§  CPU-Only Reality Check

|Model|Precision|CPU Feasible?|Realistic Experience|
|---|---|---|---|
|**SD 1.5**|**fp16**|âš ï¸ Yes (limited)|3â€“15 min per 512Ã—512 image. Some CPUs donâ€™t benefit from fp16 and may auto-cast to fp32 internally. Slightly lower RAM use.|
|**SD 1.5**|**fp32**|âš ï¸ Yes|5â€“25+ min per 512Ã—512 image. Higher RAM usage. More stable across CPU types.|
|SD 2.1|fp16/fp32|âš ï¸ Barely|15â€“40+ min per image. Often not worth it.|
|SDXL Base|fp16|âŒ Not practical|Extremely slow (30â€“90+ min). Likely memory errors.|
|SDXL Base|fp32|âŒ No|Will likely crash on RAM before finishing.|
|Flux (Dev/Pro)|mixed precision|âŒ No|Not realistic on CPU.|

If you are CPU-only:  
ğŸ‘‰ **Stick to SD 1.5 at 512x512**

---

## ğŸ† Most Compatible Overall

If your goal is:

> â€œWorks on the most computers with least dramaâ€

Choose:

- âœ… SD 1.5 fp16
- âœ… 512x512 resolution
- âœ… No ControlNet
- âœ… No refiner
- âœ… No custom nodes

---

## ğŸŸ¢ Best CPU Setup for SD 1.5 (CPU-Only)

If running **without a GPU**, use:

- âœ… **Resolution:** 512Ã—512
- âœ… **Steps:** 20 or fewer
- âœ… **Sampler:** Euler or Euler a
- âœ… **Batch size:** 1
- âœ… Disable extra nodes
- âœ… Avoid ControlNet
- âœ… Avoid SDXL

---

# âš™ï¸ fp16 vs fp32 on CPU (Important)

On CPU:

- **fp16 does NOT usually speed things up**
    
- Many CPUs internally convert fp16 â†’ fp32
    
- fp16 may reduce RAM slightly
    
- fp32 is often more stable
    

So performance difference is usually small â€” but hereâ€™s realistic timing:

---

# â± Expected Time Per Image (512Ã—512, 20 steps)

## ğŸ§  Modern i7 / Ryzen 7 (8â€“16 threads)

|Precision|Time Per Image|Notes|
|---|---|---|
|**fp16**|3â€“8 min|Slightly lower RAM. May not be faster.|
|**fp32**|4â€“10 min|Slightly more RAM. Very stable.|

---

## ğŸ§  Older i5 (4â€“6 threads)

|Precision|Time Per Image|Notes|
|---|---|---|
|**fp16**|8â€“20 min|Sometimes equal to fp32 speed.|
|**fp32**|10â€“25+ min|More RAM usage. Stable.|

---

## ğŸ§  Low-Power Laptop CPU (U-series / thin notebooks)

|Precision|Time Per Image|Notes|
|---|---|---|
|**fp16**|15â€“35 min|May thermal throttle.|
|**fp32**|20â€“40+ min|Higher RAM pressure.|

---

# ğŸ§© RAM Expectations

|Precision|System RAM Needed|
|---|---|
|fp16|8GB minimum (16GB recommended)|
|fp32|16GB recommended|

---

## ğŸ¯ Leverage AI

Tell AI:

- GPU model
- VRAM amount
- Mac or Windows
- RAM amount

And as it for ComfyUI, what are the:
- safe tier
- stretch tier
- "Donâ€™t even try locally" tier

---

*Weng's personal ChatGPT thread on developing these notes: https://chatgpt.com/c/699c3c27-9274-8328-828b-6bac8716ffff*