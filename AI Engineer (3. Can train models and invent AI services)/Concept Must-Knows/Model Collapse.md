**Model collapse** refers to a phenomenon where new AI models trained on outputs of older AI models (instead of original human-generated data) begin to degrade in quality over time. This is especially a concern in large-scale generative models like GPT, image generators, or diffusion models.

---

### ğŸ”„ How It Happens:

When an AI is trained on data that is itself generated by other AIs, you get:

- **Less diversity** in the training data
    
- **Amplified biases or errors** present in earlier models
    
- **Loss of grounding** in human reality or natural language
    
- **Reinforced artifacts**, where synthetic patterns get mistaken for real ones
    

This effect **compounds with each generation**, leading to what some researchers call a **"model collapse"** â€” where the AI becomes less intelligent or useful over time.

---

### ğŸ“Š Real-World Example:

Imagine if every new encyclopedia edition were written by summarizing the last edition without referencing the original sources. Errors, simplifications, and style quirks would accumulate and distort the information. Thatâ€™s essentially what happens in AI model collapse.

---

### ğŸ§ª Research Origin:

The term became widely discussed after a 2023 paper titled **"The Curse of Recursion: Training on Generated Data Makes Models Forget"** (Shumailov et al., 2023). They showed that even a small amount of AI-generated data in the training mix can degrade performance measurably.

---

### ğŸ›¡ï¸ Mitigation Techniques:

- **Filter AI-generated content** from training data
    
- **Prioritize human-created data** during fine-tuning
    
- **Use synthetic data sparingly** and with proper labeling
    
- **Incorporate retrieval or grounded systems** to stay linked to source material
