**Model collapse** refers to a phenomenon where new AI models trained on outputs of older AI models (instead of original human-generated data) begin to degrade in quality over time. This is especially a concern in large-scale generative models like GPT, image generators, or diffusion models.

---

### 🔄 How It Happens:

When an AI is trained on data that is itself generated by other AIs, you get:

- **Less diversity** in the training data
    
- **Amplified biases or errors** present in earlier models
    
- **Loss of grounding** in human reality or natural language
    
- **Reinforced artifacts**, where synthetic patterns get mistaken for real ones
    

This effect **compounds with each generation**, leading to what some researchers call a **"model collapse"** — where the AI becomes less intelligent or useful over time.

---

### 📊 Real-World Example:

Imagine if every new encyclopedia edition were written by summarizing the last edition without referencing the original sources. Errors, simplifications, and style quirks would accumulate and distort the information. That’s essentially what happens in AI model collapse.

---

### 🧪 Research Origin:

The term became widely discussed after a 2023 paper titled **"The Curse of Recursion: Training on Generated Data Makes Models Forget"** (Shumailov et al., 2023). They showed that even a small amount of AI-generated data in the training mix can degrade performance measurably.

---

### 🛡️ Mitigation Techniques:

- **Filter AI-generated content** from training data
    
- **Prioritize human-created data** during fine-tuning
    
- **Use synthetic data sparingly** and with proper labeling
    
- **Incorporate retrieval or grounded systems** to stay linked to source material
