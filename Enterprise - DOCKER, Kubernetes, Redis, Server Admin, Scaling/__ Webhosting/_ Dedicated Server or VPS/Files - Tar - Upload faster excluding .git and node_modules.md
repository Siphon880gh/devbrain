**Uploading many files can be slow**, especially with large folders or numerous small files. A faster approach is to first **zip the entire folder**, upload the single archive, and then **extract it using SSH** on the server. As for uploading, you have various options (Refer to )

However, this method can still be slow if your archive includes large or unnecessary directories like `.git`. To avoid this, simply **exclude `.git` and `node_modules` folders recursively** during compression to speed up the process and reduce archive size.

Archiving*
```
COPYFILE_DISABLE=1 tar -czvf archive_name.tar.gz path/to/myfolder
```

^ The environment variable `COPYFILE_DISABLE=1` is used on **macOS** to **prevent tar from including hidden `._` Apple metadata files** (also known as AppleDouble files) when creating an archive.

Unarchiving:
```
tar -xzvf archive_name.tar.gz
```


---

Archive excluding .git and node_modules recursively:
Add before `-czvf`:
`--exclude='*/.git' --exclude='*/node_modules'`

---


## Troubleshooting - Extra \_ files after unarchiving

COPYFILE_DISABLE=1 didn't work then. Look at OS' way to do it


To recursively remove all files that start with `._` in their filenames, you can use the `find` command in a Unix-based system (such as Linux or macOS). Open a terminal and run the following command:  

```sh
find /path/to/directory -name "._*" -type f -delete
```