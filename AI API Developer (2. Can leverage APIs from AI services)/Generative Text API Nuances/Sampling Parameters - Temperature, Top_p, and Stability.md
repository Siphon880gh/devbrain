> [!note] ğŸ§  **Model-Specific Parameters**  
> `temperature` and `top_p` are **sampling controls** for _OpenAI language models_ (such as ChatGPT or GPT-4). They shape the **creativity** and **diversity** of text output.  
> `stability`, by contrast, is a knob for _ElevenLabs voice models_ that governs how closely the generated speech hews to its reference tone.  
> Although all three parameters influence **variation**, they apply to **different domains**â€”text versus voiceâ€”illustrating how many AI systems let you balance **accuracy** against **creativity**.

---

When you interact with a language or voice model, you never receive â€œthe one correct answer.â€ Instead, the model samples a response from a **probability distribution** of possible outputs. Controls such as **`temperature`**, **`top_p`**, and **`stability`** let you reshape that distribution:

- **If output feels _too repetitive or robotic_**  
    raise `temperature` or widen `top_p` to encourage the model to explore novel options.
    
- **If output feels _too random or incoherent_**  
    lower `temperature` or shrink `top_p` so the model focuses on its most confident predictions.
    

Typical fine-tuning scenarios include:
- Calibrating a chatbotâ€™s tone and inventiveness
- Improving code-generation reliability
- Balancing a voice modelâ€™s expressiveness with consistency

Appreciating the **math behind these parameters**â€”how they alter token probabilities, model confidence, and domain mixingâ€”empowers you to dial in just the right blend of creativity and control.

---

## Parameter Cheat-Sheet

| Parameter                  | What it Controls                                                                                             | Practical Effect                                                            | Typical Range              | How the Math Works                                                                                                                                             | Math Concept                                                                                             | **Effect on Modelâ€™s Understanding**                                                                                                                                                                                                                       | ğŸ§  Mnemonic / Mental Hook                                                                                                                                                                                         |
| -------------------------- | ------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------- | -------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **`temperature`**Â (OpenAI) | Multiplies the logits before sampling. Lower = pick high-probability tokens, higher = flatten probabilities. | â€¢ Low: reliable, repeatable  <br>â€¢ High: creative, risky                    | `0.0 â€“ 2.0`                | Multiplies logits (pre-softmax scores) byÂ `1/temperature`. Lower temp â†’ softmax distribution becomes sharper (peaks more). Higher temp â†’ flatter distribution. | **Softmax* weighting**: Temperature controls how heavily weights are skewed toward dominant activations. | **Low temp**Â = model leans into dominant concepts (strong activations across trained weights).**High temp**Â = model weakens strong weights and allows less-relevant associations to emerge, enabling novel or unusual combinations of ideas.              | ğŸ”¥Â **â€œHot = spread out, Cold = focusedâ€**  <br>**Hotter**Â â†’ particles spread like in the ideal gas law â†’ tokens sampled from many domains  <br>**Colder**Â â†’ particles cluster â†’ model stays in one focused domain |
| **`top_p`**Â (OpenAI)       | Probability threshold for selecting token pool                                                               | â€¢ Low: cautious, constrained vocabulary  <br>â€¢ High: more varied, more risk | `0.1 â€“ 1.0`                | Uses cumulative probability to define a cutoff in token list. Tokens outside this top % are excluded.                                                          | **Nucleus sampling**: Keeps only top tokens whose combined probability â‰¥Â `top_p`.                        | **LowÂ `top_p`**Â = narrows possible directions, forces focus on most plausible next ideas.  <br>**HighÂ `top_p`**Â = increases the number of "next steps" the model might take, opening up cross-domain jumps, weaker associations, and more diverse output. | ğŸ¥§Â **â€œBiggerÂ `p`, bigger piece of the pieâ€**  <br>LowÂ `p`Â = smaller slice = tight token pool  <br>HighÂ `p`Â = larger slice = more ideas let in                                                                     |
| **Stability**Â (ElevenLabs) | Voice consistency: reference-faithful vs expressive                                                          | â€¢ Robust: on-script, no driftâ€¢ Creative: emotional, may hallucinate         | Slider: Creative â†â†’ Robust | Modulates how strictly the TTS output matches latent voice embedding.                                                                                          | **Latent space distance**: Controls drift from the â€œtrueâ€ reference vector.                              | **High stability**Â = tight control over phoneme and tone weights â€” voice stays in its lane.  <br>**Low stability**Â = model weights allow more fluctuation, enabling emotional tone, but at risk of blending with unrelated speech patterns or phrasing.   | ğŸ™ï¸Â **â€œStable like a radio signalâ€**  <br>High = locked on station  <br>Low = expressive but may drift or add static                                                                                              |


---

### Sample API Response (OpenAI)

Pay attention to the `temperature` and `top_p` fields:

```json
{
  "id": "resp_67ccd2bed1ec8190b14f964abc0542670bb6a6b452d3795b",
  "object": "response",
  "created_at": 1741476542,
  "status": "completed",
  "model": "gpt-4.1-2025-04-14",
  "temperature": 1.0,
  "top_p": 1.0,
  "output": [
    {
      "role": "assistant",
      "content": [
        {
          "type": "output_text",
          "text": "In a peaceful grove beneath a silver moon, a unicorn named Lumina discovered a hidden pool that reflected the stars..."
        }
      ]
    }
  ],
  "usage": {
    "input_tokens": 36,
    "output_tokens": 87,
    "total_tokens": 123
  }
}
```

---

## Understanding Softmax and Temperature

The **softmax function** converts a list of logits into probabilities that sum to 1â€”those probabilities form the y-axis if you graph them.

- **Low temperature** â†’ steeper softmax curve: one probability spikes while the rest collapse toward 0.
    
- **High temperature** â†’ flatter curve: probabilities spread more evenly.
    

Every y-value is the chance the model will choose **that specific token** next.

### ğŸ§  Step-By-Step Example

1. **Vocabulary**: `["The", "cat", "sat", "on", "mat", "."]`
    
2. **Logits**: `[1.3, 2.5, 0.7, 3.2, 0.2, 1.0]`
    
3. **Softmax output**: `[0.12, 0.25, 0.05, 0.40, 0.03, 0.15]`  
    _Each probability corresponds to one token._
    

> **Why no â€œS-curveâ€?**  
> The familiar sigmoid-style S-shape that you may have learned in school appears when plotting a **continuous** input. Softmax over a **categorical** token set instead yields a probability distribution, not a logistic curveâ€”but the values still sum to 1.

![[Pasted image 20250615070430.png]]

Softmax with different temperaturesâ€”standard versus flattenedâ€”looks more like this:

![[Pasted image 20250615070442.png]]