Goals:
- Read and understand

---

AI Agents in Cursor are specialized workers that automate parts of your development workflow. They run in the background, respond to triggers, transform files, and follow structured instructions. This primer explains how they work—and their limitations.

---

## **Agents Work With Local Files and GitHub**

An agent can work:

* Directly in your **local environment**
* Inside a **GitHub repo**
* Or both

This flexibility supports solo developers, teams, automation pipelines, and mixed workflows. Wherever your code lives, agents can operate on it.

---

## **Agents Run in Parallel — but Not in Coordination**

Cursor allows multiple agents to run simultaneously, each performing its own transformation. As they finish, effects simply appear—no manual babysitting required.

### **Parallel ≠ Collaborative**

Agents do **not** communicate with each other. There's no built-in mechanism like:

* *"Agent 2, wait until Agent 1 finishes this file."*
* *"This text is already translated—don't rewrite it."*

Each agent independently processes whatever falls inside its rules.

### **Example: When Agents Collide**

* **Translator Agent** converts English → Spanish
* **Personality Agent** rewrites tone

If Personality Agent expects English, it may distort the Spanish result. If their order flips, the tone rewrite might get lost during translation. Agents trigger when applicable—they don't negotiate order.

### **Example: Can Agent 1 Trigger Agent 2?**

You might want:

* **Schema Agent** generates `schema.json`
* **Client Agent** generates API client code based on the schema

Can Schema Agent tell Client Agent "I'm done, your turn"? **No.** There's no direct agent-to-agent messaging or signaling in Cursor.

However, you can achieve this indirectly—instruct Client Agent to wait for `schema.json` to exist before proceeding. This "file gate" pattern (covered in [DIY Coordination Techniques](#diy-coordination-techniques)) lets you simulate sequential handoffs without actual agent communication.

---

## **How Agents Respond to Triggers**

So how do agents know when to act? They activate in response to **triggers**—events that tell the agent "it's time."

### **Common Trigger Types**

* **File changes** — Save, create, or modify files matching patterns (e.g., `*.md`, `src/**/*.ts`)
* **Chat messages** — Messages in Cursor's chat matching the agent's scope
* **Manual invocation** — Explicitly asking the agent to run
* **Pattern matching** — Content inside a file matching specific rules or keywords

### **Example: A Linting Agent**

* **Trigger:** Any `.js` file saved in `/src`
* **Action:** Scan and suggest formatting fixes

Every save on a JavaScript file wakes the agent to inspect changes and offer corrections—no manual command needed.

### **Why Triggers Matter**

Triggers define **when** an agent acts, not just **what** it does:

* No wasted resources on irrelevant files
* Instant response to events you care about
* Fine-tuned control over which files, folders, or messages activate each agent

Think of triggers as the agent's "ears"—listening for specific signals.

---

## **Checkpoints Give You Control**

Worried an agent might push changes too far? Build **checkpoints** into the workflow.

A checkpoint is a pause where the agent stops, shows what it's done, and waits for approval before continuing. This lets you:

* Review each step
* Approve or revise
* Avoid unintended large-scale changes
* Maintain confidence in the process

You can always insert these "green-light moments" rather than one-shotting a full workflow.

---

## **Guided Automation Using Markdown Prompt Files**

Create multi-step automation by dropping **Markdown (.md) prompt files** into a folder, then tell the agent:

> "Follow the prompts in this folder in order."

The agent will:

1. Read each markdown file as an instruction
2. Execute steps sequentially
3. Apply them to specified files
4. Use checkpoints if included

This builds complex workflows without custom scripts—just give the agent a playbook.

**Examples:**

* Multi-step refactors
* Codebase cleanup workflows
* Linting + tone rewrite + documentation sequences
* Standardized project setup pipelines

### **A Note on Multi-Agent Execution**

When multiple agents run simultaneously, there is **no intelligent orchestration**. Cursor doesn't dynamically coordinate agents, resolve conflicts, or optimize execution order.

Any prioritization is based on simplistic heuristics—not "smart" scheduling that analyzes your workflow.

**Takeaway:** If agent order matters, design it yourself—chain prompts, use checkpoints, or run agents sequentially.

### **DIY Coordination Techniques**

Since Cursor won't coordinate agents for you, here are workarounds:

#### **1. File Existence as a Gate**

Have one agent wait for a file another agent creates:

* **Prompt 1:** "Generate the API schema and save to `schema.json`."
* **Prompt 2:** "Wait until `schema.json` exists, then generate client code based on it."

The second agent effectively "blocks" until the first agent's output appears.

#### **2. Shared State File for Pseudo-Parallel Coordination**

Use a shared state file like `cursor-agent-state.json`:

1. Both agents check if the file exists—if not, create it with initial flags
2. **Agent A** performs its task, then sets `"taskA_done": true`
3. **Agent B** is instructed: "When `taskA_done` is true, proceed"

```json
{
  "taskA_done": false,
  "taskB_done": false,
  "data_validated": false
}
```

Each agent reads the file, checks its flag, acts accordingly, and updates its flag when finished.

---

## **Summary**

In Cursor AI, think of AI agents as your multitasking little assistants. They can run in parallel, working on different tasks simultaneously, whether you're connecting them to a GitHub repository or having them operate on your local files.

One of the neat features is that you're not locked into a "one-shot" approach. You can set up checkpoints—points where the agent pauses and waits for your review before continuing. This gives you the peace of mind to make sure everything's on track.

You can also automate things by placing a series of prompts in markdown files inside a folder. The agent can then follow these prompts one by one in order, creating a kind of orchestrated workflow that still has checkpoints if you want them. So in a nutshell, you get a flexible, safe, and pretty smart way to run your AI workflows exactly the way you like.